{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from collections import defaultdict, OrderedDict\n",
    "from datetime import datetime\n",
    "import ollama\n",
    "import pandas as pd\n",
    "from html import unescape\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PROCESS RAW DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAW DATA to JSONs\n",
    "\n",
    "# Define the folder paths\n",
    "input_folder_path = 'Raw_data'\n",
    "output_folder_path = 'Output_JSONs'\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "# Iterate through all files in the input folder\n",
    "for file_name in os.listdir(input_folder_path):\n",
    "    if file_name.endswith('.txt'):  # Process only text files\n",
    "        file_path = os.path.join(input_folder_path, file_name)\n",
    "        \n",
    "        # Initialize a list to store JSON objects\n",
    "        json_objects = []\n",
    "\n",
    "        # Read and process the file\n",
    "        with open(file_path, 'r' , encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                # Attempt to parse each line as JSON\n",
    "                try:\n",
    "                    json_object = json.loads(line)\n",
    "                    json_objects.append(json_object)\n",
    "                except json.JSONDecodeError:\n",
    "                    # Handle or log the error if necessary\n",
    "                    pass\n",
    "\n",
    "        # Save each JSON object to a separate file with the file name as a prefix\n",
    "        base_file_name = os.path.splitext(file_name)[0]\n",
    "        for i, json_object in enumerate(json_objects):\n",
    "            output_file_path = os.path.join(output_folder_path, f'json_object_{i+1}.json')\n",
    "            with open(output_file_path, 'w') as outfile:\n",
    "                json.dump(json_object, outfile)\n",
    "\n",
    "        # Output the list of generated file paths\n",
    "        generated_files = [os.path.join(output_folder_path, f'json_object_{i+1}.json') for i in range(len(json_objects))]\n",
    "        #print(f'Generated files for {file_name}:', generated_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all keys from the JSON object\n",
    "\n",
    "# Load the JSON object from the uploaded file\n",
    "file_path = r'Output_JSONs/json_object_1.json'\n",
    "with open(file_path, 'r' , encoding='utf-8') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Function to recursively extract all keys from the JSON object\n",
    "def extract_keys(obj, keys=set()):\n",
    "    if isinstance(obj, dict):\n",
    "        for key, value in obj.items():\n",
    "            keys.add(key)\n",
    "            extract_keys(value, keys)\n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            extract_keys(item, keys)\n",
    "    return keys\n",
    "\n",
    "# Extract all keys from the JSON data\n",
    "all_keys = extract_keys(json_data)\n",
    "\n",
    "# Print all unique keys\n",
    "print(\"All keys in the JSON file:\")\n",
    "for key in all_keys:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRICES PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder paths\n",
    "input_folder_path = 'Output_JSONs'\n",
    "output_folder_path = 'Extracted_info_prices'\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "# Initialize the counter for unknown_ean files\n",
    "unknown_id_counter = 1\n",
    "\n",
    "# List to store input and output file mappings\n",
    "file_mappings = []\n",
    "\n",
    "# Iterate through all JSON files in the input folder\n",
    "for file_name in os.listdir(input_folder_path):\n",
    "    if file_name.endswith('.json'):  # Process only JSON files\n",
    "        file_path = os.path.join(input_folder_path, file_name)\n",
    "\n",
    "        # Load the JSON object from the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        # Extract the required fields\n",
    "        extracted_data = {\n",
    "            \"ean\": json_data.get(\"ean\"),\n",
    "            \"ean13\": json_data.get(\"ean13\"),\n",
    "            \"gtins\": json_data.get(\"gtins\"),\n",
    "            \"upc\": json_data.get(\"upc\"),\n",
    "            \"upca\": json_data.get(\"upca\"),\n",
    "            \"asins\": json_data.get(\"asins\", None),\n",
    "            \"prices\": json_data.get(\"prices\"),\n",
    "            \"id\": json_data.get(\"id\"),\n",
    "            \n",
    "        }\n",
    "\n",
    "        # Determine the file name using the EAN number\n",
    "        if extracted_data[\"id\"]:\n",
    "            id_number = extracted_data[\"id\"]\n",
    "        else:\n",
    "            id_number = f\"unknown_id_{unknown_id_counter}\"\n",
    "            unknown_id_counter += 1\n",
    "        \n",
    "        output_file_name = f'{id_number}.json'\n",
    "        output_file_path = os.path.join(output_folder_path, output_file_name)\n",
    "        \n",
    "        # Save the extracted data to a new JSON file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(extracted_data, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "        # Store the input and output filenames\n",
    "        file_mappings.append({\"input_filename\": file_name, \"output_filename\": output_file_name})\n",
    "\n",
    "# Convert the file mappings to a DataFrame\n",
    "df = pd.DataFrame(file_mappings)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "excel_output_path = os.path.join(output_folder_path, 'file_mappings.xlsx')\n",
    "df.to_excel(excel_output_path, index=False)\n",
    "\n",
    "# Uncomment the line below if you want to print the path of the Excel file\n",
    "# print(f'Excel file saved to {excel_output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Define the folder paths\n",
    "input_folder_path = 'Extracted_info_prices'\n",
    "corrupted_folder_path = 'Data_with_no_prices'\n",
    "os.makedirs(corrupted_folder_path, exist_ok=True)\n",
    "\n",
    "# Iterate through all JSON files in the input folder\n",
    "for file_name in os.listdir(input_folder_path):\n",
    "    if file_name.endswith('.json'):  # Process only JSON files\n",
    "        file_path = os.path.join(input_folder_path, file_name)\n",
    "        \n",
    "        try:\n",
    "            # Load the JSON object from the file\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                json_data = json.load(file)\n",
    "\n",
    "            # Check if 'prices' exists and is a list\n",
    "            prices = json_data.get('prices', [])\n",
    "            if prices is None or not isinstance(prices, list):\n",
    "                raise TypeError(\"'prices' is not a list or is None\")\n",
    "            \n",
    "        except (TypeError, json.JSONDecodeError, KeyError) as e:\n",
    "            # Move the corrupted file to the 'Corrupted_JSONs' folder\n",
    "            corrupted_file_path = os.path.join(corrupted_folder_path, file_name)\n",
    "            shutil.move(file_path, corrupted_file_path)\n",
    "            print(f\"Moved corrupted file {file_name} to {corrupted_folder_path} due to error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder paths\n",
    "input_folder_path = 'Extracted_info_prices'\n",
    "output_folder_path = 'Pricing_history'\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "# Helper function to extract the base domain from a URL\n",
    "def get_base_domain(url):\n",
    "    try:\n",
    "        parsed_url = urlparse(url)\n",
    "        domain = parsed_url.netloc\n",
    "        if domain.startswith('www.'):\n",
    "            domain = domain[4:]\n",
    "        return domain\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Helper function to clean the merchant name\n",
    "def clean_merchant_name(name):\n",
    "    if name:\n",
    "        name = name.split('Learn more')[0]\n",
    "        name = name.split('Store info')[0]\n",
    "    return name.strip() if name else None\n",
    "\n",
    "# Helper function to parse ISO format with different variations\n",
    "def parse_iso_date(date_str):\n",
    "    for fmt in (\"%Y-%m-%dT%H:%M:%S.%fZ\", \"%Y-%m-%dT%H:%M:%S.%f\", \"%Y-%m-%dT%H:%M:%SZ\", \"%Y-%m-%dT%H:%M:%S\"):\n",
    "        try:\n",
    "            return datetime.strptime(date_str, fmt)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    raise ValueError(f\"Unknown date format: {date_str}\")\n",
    "\n",
    "# Iterate through all JSON files in the input folder\n",
    "unique_merchants = set()\n",
    "for file_name in os.listdir(input_folder_path):\n",
    "    if file_name.endswith('.json'):  # Process only JSON files\n",
    "        file_path = os.path.join(input_folder_path, file_name)\n",
    "        \n",
    "        # Load the JSON object from the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        # Dictionary to hold aggregated data\n",
    "        merchant_data = defaultdict(lambda: {\n",
    "            \"amountMax\": float('-inf'),\n",
    "            \"amountMin\": float('inf'),\n",
    "            \"amounts\": defaultdict(list),\n",
    "            \"firstDateSeen\": None,\n",
    "            \"lastDateSeen\": None,\n",
    "            \"sourceURLs\": set()\n",
    "        })\n",
    "\n",
    "        # Iterate through each price entry and aggregate data\n",
    "        for price_entry in json_data.get('prices', []):\n",
    "            merchant = clean_merchant_name(price_entry.get('merchant'))\n",
    "            source_urls = price_entry.get('sourceURLs', [])\n",
    "            \n",
    "            if not merchant:\n",
    "                if len(source_urls) == 1:\n",
    "                    merchant = get_base_domain(source_urls[0])\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            merchant_key = merchant.lower()\n",
    "\n",
    "            merchant_data[merchant_key][\"amountMax\"] = max(merchant_data[merchant_key][\"amountMax\"], price_entry[\"amountMax\"])\n",
    "            merchant_data[merchant_key][\"amountMin\"] = min(merchant_data[merchant_key][\"amountMin\"], price_entry[\"amountMin\"])\n",
    "            amount = price_entry[\"amountMax\"]\n",
    "            merchant_data[merchant_key][\"amounts\"][amount].extend(price_entry[\"dateSeen\"])\n",
    "            \n",
    "            try:\n",
    "                first_date_seen = parse_iso_date(price_entry[\"firstDateSeen\"])\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    first_date_seen = parse_iso_date(price_entry[\"lastDateSeen\"])\n",
    "                except KeyError:\n",
    "                    first_date_seen = min(parse_iso_date(date) for date in price_entry[\"dateSeen\"])\n",
    "\n",
    "            try:\n",
    "                last_date_seen = parse_iso_date(price_entry[\"lastDateSeen\"])\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    last_date_seen = parse_iso_date(price_entry[\"firstDateSeen\"])\n",
    "                except KeyError:\n",
    "                    last_date_seen = max(parse_iso_date(date) for date in price_entry[\"dateSeen\"])\n",
    "                  \n",
    "            if not merchant_data[merchant_key][\"firstDateSeen\"] or first_date_seen < merchant_data[merchant_key][\"firstDateSeen\"]:\n",
    "                merchant_data[merchant_key][\"firstDateSeen\"] = first_date_seen\n",
    "            if not merchant_data[merchant_key][\"lastDateSeen\"] or last_date_seen > merchant_data[merchant_key][\"lastDateSeen\"]:\n",
    "                merchant_data[merchant_key][\"lastDateSeen\"] = last_date_seen\n",
    "            \n",
    "            merchant_data[merchant_key][\"sourceURLs\"].update([get_base_domain(url) for url in source_urls if get_base_domain(url)])\n",
    "\n",
    "        # Collect unique merchants\n",
    "        unique_merchants.update(merchant_data.keys())\n",
    "\n",
    "        # Prepare the final extracted data\n",
    "        final_data = {\n",
    "            \"ean\": json_data.get(\"ean\"),\n",
    "            \"ean13\": json_data.get(\"ean13\"),\n",
    "            \"upc\": json_data.get(\"upc\"),\n",
    "            \"upca\": json_data.get(\"upca\"),\n",
    "            \"gtins\": json_data.get(\"gtins\"),\n",
    "            \"asins\": json_data.get(\"asins\", None),\n",
    "            \"id\": json_data.get(\"id\"),\n",
    "            \"prices\": []\n",
    "        }\n",
    "\n",
    "        for merchant_key, data in merchant_data.items():\n",
    "            price_data = {\n",
    "                \"merchant\": merchant_key,\n",
    "                \"amountMax\": data[\"amountMax\"],\n",
    "                \"amountMin\": data[\"amountMin\"],\n",
    "                \"firstDateSeen\": data[\"firstDateSeen\"].isoformat(),\n",
    "                \"lastDateSeen\": data[\"lastDateSeen\"].isoformat(),\n",
    "                \"sourceURLs\": list(data[\"sourceURLs\"])\n",
    "            }\n",
    "            for i, (amount, dates) in enumerate(data[\"amounts\"].items()):\n",
    "                price_data[f\"currency_{i}\" if i > 0 else \"currency\"] = amount\n",
    "                price_data[f\"dateSeen_{i}\" if i > 0 else \"dateSeen\"] = sorted(dates)\n",
    "            final_data[\"prices\"].append(price_data)\n",
    "\n",
    "        # Save the extracted data to a new JSON file named with the EAN number\n",
    "        if json_data is None or json_data.get(\"id\") is None:\n",
    "            continue\n",
    "\n",
    "        # Skip files with unknown id number\n",
    "        id_number = json_data[\"id\"]\n",
    "        # Save the updated JSON file to the output folder\n",
    "        output_file_path = os.path.join(output_folder_path, f'{id_number}.json')\n",
    "        with open(output_file_path, 'w') as outfile:\n",
    "            json.dump(final_data, outfile, indent=2)\n",
    "\n",
    "        print(f'Extracted data saved to {output_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_price(merchant_data):\n",
    "    latest_date = None\n",
    "    latest_currency = None\n",
    "    \n",
    "    # Iterate over dateSeen_0, dateSeen_1, etc., until we don't find a key\n",
    "    for i in range(1000000):  # Arbitrarily large number, assuming the maximum number of entries\n",
    "        key = f\"dateSeen_{i}\"\n",
    "        if key in merchant_data and merchant_data[key]:\n",
    "            dates = merchant_data[key]\n",
    "            if dates:\n",
    "                last_date = max(dates)\n",
    "                if not latest_date or last_date > latest_date:\n",
    "                    latest_date = last_date\n",
    "                    currency_key = f\"currency_{i}\"\n",
    "                    latest_currency = merchant_data[currency_key] if currency_key in merchant_data else None\n",
    "    \n",
    "    return latest_date, latest_currency\n",
    "\n",
    "def process_data(input_data):\n",
    "    ean = input_data.get(\"ean\", [None])[0]\n",
    "    ean13 = input_data.get(\"ean13\")\n",
    "    upc = input_data.get(\"upc\")\n",
    "    upca = input_data.get(\"upca\")\n",
    "    gtins = input_data.get(\"gtins\")\n",
    "    asins = input_data.get(\"asins\", None)\n",
    "    id_number = input_data.get(\"id\")\n",
    "    \n",
    "    prices = input_data[\"prices\"]\n",
    "    result = {\n",
    "        \"ean\": [ean],\n",
    "        \"ean13\": ean13,\n",
    "        \"upca\": upca,\n",
    "        \"upc\": upc,\n",
    "        \"gtins\": gtins,\n",
    "        \"asins\": asins,\n",
    "        \"id\": id_number,\n",
    "        \"prices\": []\n",
    "    }\n",
    "    \n",
    "    for price in prices:\n",
    "        merchant = price[\"merchant\"]\n",
    "        latest_date, latest_currency = find_latest_price(price)\n",
    "        \n",
    "        if latest_date and latest_currency:\n",
    "            result[\"prices\"].append({\n",
    "                \"merchant\": merchant,\n",
    "                \"currency\": latest_currency,\n",
    "                \"dateSeen\": latest_date,\n",
    "                \"sourceURLs\": price.get(\"sourceURLs\", [])\n",
    "            })\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_directory = \"Latest_Pricing_per_Merchant\"\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Directory containing input JSON files\n",
    "input_directory = \"Pricing_history\"\n",
    "\n",
    "# Iterate through all JSON files in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        input_file = os.path.join(input_directory, filename)\n",
    "        \n",
    "        # Load input data from file\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            input_data = json.load(f)\n",
    "        \n",
    "        # Process the data\n",
    "        output_data = process_data(input_data)\n",
    "        \n",
    "        # Define output file path\n",
    "        output_file = os.path.join(output_directory, f\"{output_data['id']}.json\")\n",
    "        \n",
    "        # Save the output to a new JSON file\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(output_data, f, indent=2)\n",
    "        \n",
    "        print(f\"Processed {filename} and saved output to {output_file}\")\n",
    "\n",
    "print(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = \"Latest_Pricing_per_Merchant\"\n",
    "\n",
    "# Create output directory for 2024 data\n",
    "output_directory = \"Latest_Pricing_per_Merchant_2024\"\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Function to filter entries where dateSeen begins with \"2024\" and get the latest entry for each merchant\n",
    "def filter_2024_data(input_data):\n",
    "    ean = input_data.get(\"ean\", [None])[0]\n",
    "    ean13 = input_data.get(\"ean13\")\n",
    "    upc = input_data.get(\"upc\")\n",
    "    upca = input_data.get(\"upca\")\n",
    "    gtins = input_data.get(\"gtins\")\n",
    "    asins = input_data.get(\"asins\", None)\n",
    "    id_number = input_data.get(\"id\")\n",
    "\n",
    "    prices = input_data[\"prices\"]\n",
    "\n",
    "    # Dictionary to store the latest entry for each merchant\n",
    "    filtered_prices = {}\n",
    "    for price in prices:\n",
    "        if price[\"dateSeen\"].startswith(\"2024\"):\n",
    "            merchant = price[\"merchant\"]\n",
    "            date_seen = price[\"dateSeen\"]\n",
    "\n",
    "            if merchant not in filtered_prices or datetime.strptime(date_seen, '%Y-%m-%d') > datetime.strptime(filtered_prices[merchant][\"dateSeen\"], '%Y-%m-%d'):\n",
    "                filtered_prices[merchant] = price\n",
    "\n",
    "    return {\n",
    "        \"ean\": [ean],\n",
    "        \"ean13\": ean13,\n",
    "        \"upc\": upc,\n",
    "        \"upca\": upca,\n",
    "        \"gtins\": gtins,\n",
    "        \"asins\": asins,\n",
    "        \"id\": id_number,\n",
    "        \"prices\": list(filtered_prices.values())\n",
    "    }\n",
    "\n",
    "# Iterate through all JSON files in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".json\"):\n",
    "        input_file = os.path.join(input_directory, filename)\n",
    "\n",
    "        # Load input data from file\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            input_data = json.load(f)\n",
    "\n",
    "        # Filter the data\n",
    "        filtered_data = filter_2024_data(input_data)\n",
    "\n",
    "        # Skip files where no data is available for 2024\n",
    "        if not filtered_data[\"prices\"]:\n",
    "            continue\n",
    "\n",
    "        # Define output file path\n",
    "        output_file = os.path.join(output_directory, filename)\n",
    "\n",
    "        # Save the output to a new JSON file\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(filtered_data, f, indent=2)\n",
    "\n",
    "        print(f\"Processed {filename} and saved output to {output_file}\")\n",
    "\n",
    "print(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_merchant_names(file_path):\n",
    "    replacements = {\n",
    "        'amazon': 'amazon',\n",
    "        'walmart': 'walmart',\n",
    "        'ebay': 'ebay',\n",
    "        'bestbuy': 'bestbuy',\n",
    "        'best buy': 'bestbuy',\n",
    "        'kmart': 'kmart',\n",
    "        'target': 'target',\n",
    "        'newegg': 'newegg',\n",
    "        'new egg': 'newegg',\n",
    "        'overstock': 'overstock',\n",
    "        'lowes': 'lowes',\n",
    "        'lowe\\'s': 'lowes',\n",
    "        'homedepot': 'homedepot',\n",
    "        'home depot': 'homedepot',\n",
    "        'bonanza': 'bonanza',\n",
    "        'sears': 'sears',\n",
    "        'kohl': 'kohls',\n",
    "        'artfire': 'artfire',\n",
    "        'bkstr': 'bkstr',\n",
    "        'buya.com': 'buya',\n",
    "        'discount bandit': 'discount bandit',\n",
    "        'ebluejay': 'ebluejay',\n",
    "        'ecrater': 'ecrater',\n",
    "        'truegether': 'truegether',\n",
    "        'macys': 'macys',\n",
    "        'macy\\'s': 'macys',\n",
    "        'macys.com': 'macys',\n",
    "    }\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    updated_prices = []\n",
    "    \n",
    "    for item in data.get('prices', []):\n",
    "        original_name = item.get('merchant', '').lower()\n",
    "        for key, value in replacements.items():\n",
    "            if key in original_name:\n",
    "                item['merchant'] = value\n",
    "                break\n",
    "        updated_prices.append(item)\n",
    "    \n",
    "    return data, updated_prices\n",
    "\n",
    "def process_directory(input_directory, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith('.json'):\n",
    "            file_path = os.path.join(input_directory, file_name)\n",
    "            original_data, updated_prices = replace_merchant_names(file_path)\n",
    "            \n",
    "            # Grouping data by merchant name\n",
    "            grouped_data = {}\n",
    "            for item in updated_prices:\n",
    "                merchant = item.get('merchant')\n",
    "                if merchant not in grouped_data:\n",
    "                    grouped_data[merchant] = []\n",
    "                grouped_data[merchant].append(item)\n",
    "            \n",
    "            # Updating the original data with the grouped prices\n",
    "            original_data['prices'] = [item for sublist in grouped_data.values() for item in sublist]\n",
    "            \n",
    "            # Writing the updated data to a new JSON file\n",
    "            output_file_path = os.path.join(output_directory, file_name)\n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                json.dump(original_data, output_file, indent=4)\n",
    "            \n",
    "            #print(f\"Processed and saved {file_name} to {output_directory}\")\n",
    "\n",
    "# Replace with the actual directory paths\n",
    "input_directory = 'Pricing_history'\n",
    "output_directory = 'output'\n",
    "\n",
    "process_directory(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_merchant_names(file_path):\n",
    "    replacements = {\n",
    "        'amazon': 'amazon',\n",
    "        'walmart': 'walmart',\n",
    "        'ebay': 'ebay',\n",
    "        'bestbuy': 'bestbuy',\n",
    "        'best buy': 'bestbuy',\n",
    "        'kmart': 'kmart',\n",
    "        'target': 'target',\n",
    "        'newegg': 'newegg',\n",
    "        'new egg': 'newegg',\n",
    "        'overstock': 'overstock',\n",
    "        'lowes': 'lowes',\n",
    "        'lowe\\'s': 'lowes',\n",
    "        'homedepot': 'homedepot',\n",
    "        'home depot': 'homedepot',\n",
    "        'bonanza': 'bonanza',\n",
    "        'sears': 'sears',\n",
    "        'kohl': 'kohls',\n",
    "        'artfire': 'artfire',\n",
    "        'bkstr': 'bkstr',\n",
    "        'buya.com': 'buya',\n",
    "        'discount bandit': 'discount bandit',\n",
    "        'ebluejay': 'ebluejay',\n",
    "        'ecrater': 'ecrater',\n",
    "        'truegether': 'truegether',\n",
    "        'macys': 'macys',\n",
    "        'macy\\'s': 'macys',\n",
    "        'macys.com': 'macys',\n",
    "    }\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    updated_prices = []\n",
    "    \n",
    "    for item in data.get('prices', []):\n",
    "        original_name = item.get('merchant', '').lower()\n",
    "        for key, value in replacements.items():\n",
    "            if key in original_name:\n",
    "                item['merchant'] = value\n",
    "                break\n",
    "        updated_prices.append(item)\n",
    "    \n",
    "    return data, updated_prices\n",
    "\n",
    "def process_directory(input_directory, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith('.json'):\n",
    "            file_path = os.path.join(input_directory, file_name)\n",
    "            original_data, updated_prices = replace_merchant_names(file_path)\n",
    "            \n",
    "            # Grouping data by merchant name\n",
    "            grouped_data = {}\n",
    "            for item in updated_prices:\n",
    "                merchant = item.get('merchant')\n",
    "                if merchant not in grouped_data:\n",
    "                    grouped_data[merchant] = []\n",
    "                grouped_data[merchant].append(item)\n",
    "            \n",
    "            # Updating the original data with the grouped prices\n",
    "            original_data['prices'] = [item for sublist in grouped_data.values() for item in sublist]\n",
    "            \n",
    "            # Writing the updated data to a new JSON file\n",
    "            output_file_path = os.path.join(output_directory, file_name)\n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                json.dump(original_data, output_file, indent=4)\n",
    "            \n",
    "            #print(f\"Processed and saved {file_name} to {output_directory}\")\n",
    "\n",
    "# Replace with the actual directory paths\n",
    "input_directory = 'Latest_Pricing_per_Merchant'\n",
    "output_directory = 'Latest_Pricing_per_Merchant_v2'\n",
    "\n",
    "process_directory(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_merchant_names(file_path):\n",
    "    replacements = {\n",
    "        'amazon': 'amazon',\n",
    "        'walmart': 'walmart',\n",
    "        'ebay': 'ebay',\n",
    "        'bestbuy': 'bestbuy',\n",
    "        'best buy': 'bestbuy',\n",
    "        'kmart': 'kmart',\n",
    "        'target': 'target',\n",
    "        'newegg': 'newegg',\n",
    "        'new egg': 'newegg',\n",
    "        'overstock': 'overstock',\n",
    "        'lowes': 'lowes',\n",
    "        'lowe\\'s': 'lowes',\n",
    "        'homedepot': 'homedepot',\n",
    "        'home depot': 'homedepot',\n",
    "        'bonanza': 'bonanza',\n",
    "        'sears': 'sears',\n",
    "        'kohl': 'kohls',\n",
    "        'artfire': 'artfire',\n",
    "        'bkstr': 'bkstr',\n",
    "        'buya.com': 'buya',\n",
    "        'discount bandit': 'discount bandit',\n",
    "        'ebluejay': 'ebluejay',\n",
    "        'ecrater': 'ecrater',\n",
    "        'truegether': 'truegether',\n",
    "        'macys': 'macys',\n",
    "        'macy\\'s': 'macys',\n",
    "        'macys.com': 'macys',\n",
    "    }\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    updated_prices = []\n",
    "    \n",
    "    for item in data.get('prices', []):\n",
    "        original_name = item.get('merchant', '').lower()\n",
    "        for key, value in replacements.items():\n",
    "            if key in original_name:\n",
    "                item['merchant'] = value\n",
    "                break\n",
    "        updated_prices.append(item)\n",
    "    \n",
    "    return data, updated_prices\n",
    "\n",
    "def process_directory(input_directory, output_directory):\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for file_name in os.listdir(input_directory):\n",
    "        if file_name.endswith('.json'):\n",
    "            file_path = os.path.join(input_directory, file_name)\n",
    "            original_data, updated_prices = replace_merchant_names(file_path)\n",
    "            \n",
    "            # Grouping data by merchant name\n",
    "            grouped_data = {}\n",
    "            for item in updated_prices:\n",
    "                merchant = item.get('merchant')\n",
    "                if merchant not in grouped_data:\n",
    "                    grouped_data[merchant] = []\n",
    "                grouped_data[merchant].append(item)\n",
    "            \n",
    "            # Updating the original data with the grouped prices\n",
    "            original_data['prices'] = [item for sublist in grouped_data.values() for item in sublist]\n",
    "            \n",
    "            # Writing the updated data to a new JSON file\n",
    "            output_file_path = os.path.join(output_directory, file_name)\n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                json.dump(original_data, output_file, indent=4)\n",
    "            \n",
    "            #print(f\"Processed and saved {file_name} to {output_directory}\")\n",
    "\n",
    "# Replace with the actual directory paths\n",
    "input_directory = 'Latest_Pricing_per_Merchant_2024'\n",
    "output_directory = 'Latest_Pricing_per_Merchant_2024_v2'\n",
    "\n",
    "process_directory(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_prices(prices):\n",
    "    merged = {\n",
    "        \"amountMax\": 0,\n",
    "        \"amountMin\": float('inf'),\n",
    "        \"firstDateSeen\": None,\n",
    "        \"lastDateSeen\": None,\n",
    "        \"sourceURLs\": set(),\n",
    "        \"currency_prices\": []\n",
    "    }\n",
    "\n",
    "    for price in prices:\n",
    "        merged[\"amountMax\"] = max(merged[\"amountMax\"], price[\"amountMax\"])\n",
    "        merged[\"amountMin\"] = min(merged[\"amountMin\"], price[\"amountMin\"])\n",
    "        if not merged[\"firstDateSeen\"] or price[\"firstDateSeen\"] < merged[\"firstDateSeen\"]:\n",
    "            merged[\"firstDateSeen\"] = price[\"firstDateSeen\"]\n",
    "        if not merged[\"lastDateSeen\"] or price[\"lastDateSeen\"] > merged[\"lastDateSeen\"]:\n",
    "            merged[\"lastDateSeen\"] = price[\"lastDateSeen\"]\n",
    "        merged[\"sourceURLs\"].update(price[\"sourceURLs\"])\n",
    "\n",
    "        for key, value in price.items():\n",
    "            if key.startswith(\"currency_\"):\n",
    "                currency_index = int(key.split('_')[-1])\n",
    "                merged[\"currency_prices\"].append({\n",
    "                    \"currency\": value,\n",
    "                    \"dateSeen\": price[f\"dateSeen_{currency_index}\"]\n",
    "                })\n",
    "            elif key == \"currency\":\n",
    "                merged[\"currency_prices\"].append({\n",
    "                    \"currency\": value,\n",
    "                    \"dateSeen\": price[\"dateSeen\"]\n",
    "                })\n",
    "\n",
    "    merged[\"sourceURLs\"] = list(merged[\"sourceURLs\"])\n",
    "    return merged\n",
    "\n",
    "def group_merchants_data(input_directory, output_directory):\n",
    "    # Ensure the output directory exists\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "    # Process each JSON file in the input directory\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith('.json'):\n",
    "            input_file = os.path.join(input_directory, filename)\n",
    "            with open(input_file, 'r') as file:\n",
    "                data = json.load(file)\n",
    "            \n",
    "            grouped_data = {}\n",
    "\n",
    "            # Group data by merchant name\n",
    "            for price_entry in data['prices']:\n",
    "                merchant = price_entry['merchant']\n",
    "                if merchant not in grouped_data:\n",
    "                    grouped_data[merchant] = []\n",
    "                grouped_data[merchant].append(price_entry)\n",
    "            \n",
    "            # Create the new grouped structure for the prices\n",
    "            new_prices = []\n",
    "            for merchant, prices in grouped_data.items():\n",
    "                merged_prices = merge_prices(prices)\n",
    "                new_prices.append({\n",
    "                    \"merchant\": merchant,\n",
    "                    \"prices\": [merged_prices]\n",
    "                })\n",
    "            \n",
    "            # Preserve the original data and update the prices field\n",
    "            new_data = data.copy()\n",
    "            new_data['prices'] = new_prices\n",
    "\n",
    "            # Save the modified data to a new JSON file\n",
    "            output_file = os.path.join(output_directory, f\"{os.path.splitext(filename)[0]}_modified.json\")\n",
    "            \n",
    "            with open(output_file, 'w') as file:\n",
    "                json.dump(new_data, file, indent=4)\n",
    "            \n",
    "            #print(f\"Modified data saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_directory = 'output'  # Replace with your input directory path\n",
    "output_directory = 'output_v1'  # Replace with your output directory path\n",
    "group_merchants_data(input_directory, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_currency_prices(currency_prices):\n",
    "    transformed = []\n",
    "    for idx, price in enumerate(currency_prices, start=1):\n",
    "        new_price = {\n",
    "            f\"currency_{idx}\": price[\"currency\"],\n",
    "            f\"dateSeen_{idx}\": price[\"dateSeen\"]\n",
    "        }\n",
    "        transformed.append(new_price)\n",
    "    return transformed\n",
    "\n",
    "def transform_prices(prices):\n",
    "    for price_entry in prices:\n",
    "        if \"currency_prices\" in price_entry:\n",
    "            price_entry[\"currency_prices\"] = transform_currency_prices(price_entry[\"currency_prices\"])\n",
    "    return prices\n",
    "\n",
    "def transform_json(data):\n",
    "    for price_entry in data[\"prices\"]:\n",
    "        price_entry[\"prices\"] = transform_prices(price_entry[\"prices\"])\n",
    "    return data\n",
    "\n",
    "def process_files(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            input_file_path = os.path.join(input_dir, filename)\n",
    "            output_file_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            with open(input_file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            transformed_data = transform_json(data)\n",
    "\n",
    "            with open(output_file_path, 'w') as f:\n",
    "                json.dump(transformed_data, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_dir = 'output_v1'  # Replace with your input directory path\n",
    "    output_dir = 'output_v2'  # Replace with your output directory path\n",
    "    process_files(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def rename_files(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('_modified.json'):\n",
    "            new_filename = filename.replace('_modified', '')\n",
    "            old_filepath = os.path.join(directory, filename)\n",
    "            new_filepath = os.path.join(directory, new_filename)\n",
    "            os.rename(old_filepath, new_filepath)\n",
    "            #print(f\"Renamed '{filename}' to '{new_filename}'\")\n",
    "\n",
    "# Directory containing the JSON files\n",
    "directory = 'output_v2'  # Replace with your directory path\n",
    "\n",
    "# Rename the files\n",
    "rename_files(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_folders(folders):\n",
    "    for folder in folders:\n",
    "        if os.path.exists(folder):\n",
    "            shutil.rmtree(folder)\n",
    "            print(f\"Deleted folder '{folder}'\")\n",
    "        else:\n",
    "            print(f\"Folder '{folder}' does not exist\")\n",
    "\n",
    "def rename_folder(old_name, new_name):\n",
    "    if os.path.exists(old_name):\n",
    "        os.rename(old_name, new_name)\n",
    "        print(f\"Renamed folder '{old_name}' to '{new_name}'\")\n",
    "    else:\n",
    "        print(f\"Folder '{old_name}' does not exist\")\n",
    "\n",
    "# Folders to delete\n",
    "folders_to_delete = ['output', 'output_v1']\n",
    "\n",
    "# Folder to rename\n",
    "old_folder_name = 'output_v2'\n",
    "new_folder_name = 'Pricing_history_v2'\n",
    "\n",
    "# Delete specified folders\n",
    "delete_folders(folders_to_delete)\n",
    "\n",
    "# Rename the folder\n",
    "rename_folder(old_folder_name, new_folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURES Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder paths\n",
    "input_folder_path = 'Output_JSONs'\n",
    "output_folder_path = 'Extracted_features'\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "# Initialize the counter for unknown EANs\n",
    "unknown_id_counter = 1\n",
    "\n",
    "# Iterate through all JSON files in the input folder\n",
    "for file_name in os.listdir(input_folder_path):\n",
    "    if file_name.endswith('.json'):  # Process only JSON files\n",
    "        file_path = os.path.join(input_folder_path, file_name)\n",
    "\n",
    "        # Load the JSON object from the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        # Extract the required fields\n",
    "        extracted_data = {\n",
    "            \"brand\": json_data.get(\"brand\"),\n",
    "            \"category\": json_data.get(\"categories\"),\n",
    "            \"features\": [\n",
    "                {\n",
    "                    \"key\": feature.get(\"key\"),\n",
    "                    \"value\": feature.get(\"value\")\n",
    "                } for feature in json_data.get(\"features\", [])\n",
    "            ],\n",
    "            \"ean\": json_data.get(\"ean\"),\n",
    "            \"ean13\": json_data.get(\"ean13\"),\n",
    "            \"gtins\": json_data.get(\"gtins\"),\n",
    "            \"upc\": json_data.get(\"upc\"),\n",
    "            \"upca\": json_data.get(\"upca\"),\n",
    "            \"asins\": json_data.get(\"asins\", None),\n",
    "            \"id\": json_data.get(\"id\"),\n",
    "            \"taxonomy\": json_data.get(\"taxonomy\"),\n",
    "        }\n",
    "\n",
    "        # Determine the EAN number or use a counter for unknown EANs\n",
    "        if extracted_data[\"id\"]:\n",
    "            id_number = extracted_data[\"id\"]\n",
    "        else:\n",
    "            id_number_number = f\"unknown_id_{unknown_id_counter}\"\n",
    "            unknown_id_counter += 1\n",
    "\n",
    "        # Save the extracted data to a new JSON file named with the EAN number\n",
    "        output_file_path = os.path.join(output_folder_path, f'{id_number}.json')\n",
    "        with open(output_file_path, 'w') as outfile:\n",
    "            json.dump(extracted_data, outfile, indent=2)\n",
    "\n",
    "        print(f'Extracted data saved to {output_file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRODUCT ID PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder paths\n",
    "input_folder_path = 'Output_JSONs'\n",
    "output_folder_path = 'ID_JSONs'\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "# Initialize the counter for unknown EANs\n",
    "unknown_id_counter = 1\n",
    "\n",
    "# Iterate through all JSON files in the input folder\n",
    "for file_name in os.listdir(input_folder_path):\n",
    "    if file_name.endswith('.json'):  # Process only JSON files\n",
    "        file_path = os.path.join(input_folder_path, file_name)\n",
    "\n",
    "        # Load the JSON object from the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        # Extract the required fields\n",
    "        extracted_data = {\n",
    "            \"ean\": json_data.get(\"ean\"),\n",
    "            \"ean13\": json_data.get(\"ean13\"),\n",
    "            \"gtins\": json_data.get(\"gtins\"),\n",
    "            \"upca\": json_data.get(\"upca\"),\n",
    "            \"upc\": json_data.get(\"upc\"),\n",
    "            \"id\": json_data.get(\"id\"),\n",
    "            # Get asins from json_data.get(\"asins\") if it exists, otherwise use None\n",
    "            \"asins\": json_data.get(\"asins\", None)\n",
    "        }\n",
    "\n",
    "        # Determine the file name using the EAN number or use a counter for unknown EANs\n",
    "        if extracted_data[\"id\"]:\n",
    "            id_number = extracted_data[\"id\"]\n",
    "        else:\n",
    "            id_number = f\"unknown_id_{unknown_id_counter}\"\n",
    "            unknown_id_counter += 1\n",
    "\n",
    "        # Save the extracted data to a new JSON file\n",
    "        output_file_path = os.path.join(output_folder_path, f'{id_number}.json')\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(extracted_data, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "        # Uncomment the line below if you want to print the output file path\n",
    "        # print(f'Extracted data saved to {output_file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMAGE URLS Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the folder paths\n",
    "input_folder_path = 'Output_JSONs'\n",
    "output_folder_path = 'ImageURLs_JSONs'\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "# Initialize the counter for unknown EANs\n",
    "unknown_id_counter = 1\n",
    "\n",
    "# Iterate through all JSON files in the input folder\n",
    "for file_name in os.listdir(input_folder_path):\n",
    "    if file_name.endswith('.json'):  # Process only JSON files\n",
    "        file_path = os.path.join(input_folder_path, file_name)\n",
    "\n",
    "        # Load the JSON object from the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        # Extract the required fields\n",
    "        extracted_data = {\n",
    "            \"ean\": json_data.get(\"ean\"),\n",
    "            \"ean13\": json_data.get(\"ean13\"),\n",
    "            \"gtins\": json_data.get(\"gtins\"),\n",
    "            \"upc\": json_data.get(\"upc\"),\n",
    "            \"upca\": json_data.get(\"upca\"),\n",
    "            # Get asins from json_data.get(\"asins\") if it exists, otherwise use None\n",
    "            \"asins\": json_data.get(\"asins\", None),\n",
    "            \"id\": json_data.get(\"id\"),\n",
    "            \"imageURLs\": json_data.get(\"imageURLs\", None)\n",
    "        }\n",
    "\n",
    "        # Determine the file name using the EAN number or use a counter for unknown EANs\n",
    "        if extracted_data[\"id\"]:\n",
    "            id_number = extracted_data[\"id\"]\n",
    "        else:\n",
    "            id_number = f\"unknown_id_{unknown_id_counter}\"\n",
    "            unknown_id_counter += 1\n",
    "\n",
    "        # Save the extracted data to a new JSON file\n",
    "        output_file_path = os.path.join(output_folder_path, f'{id_number}.json')\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(extracted_data, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "        # Uncomment the line below if you want to print the output file path\n",
    "        # print(f'Extracted data saved to {output_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Merchant patterns\n",
    "merchant_patterns = {\n",
    "    'bestbuy': re.compile(r'bbystatic\\.com'),\n",
    "    'lowes': re.compile(r'lowes\\.com'),\n",
    "    'walmart': re.compile(r'walmartimages\\.com'),\n",
    "    'newegg': re.compile(r'neweggimages\\.com'),\n",
    "    'costco': re.compile(r'costco\\.com'),\n",
    "    'kroger': re.compile(r'kroger\\.com'),\n",
    "    'amazon': re.compile(r'amazon\\.com'),\n",
    "    'officedepot': re.compile(r'officedepot\\.com'),\n",
    "    'homedepot': re.compile(r'homedepot\\.com' | r'homedepotstatic\\.com'),\n",
    "    'target': re.compile(r'target\\.scene7\\.com'),\n",
    "    'ebay': re.compile(r'ebayimg\\.com'),\n",
    "    'ajmadison': re.compile(r'ajmadison\\.com'),\n",
    "    'wayfair': re.compile(r'wayfairimages\\.com'),\n",
    "    'overstock': re.compile(r'overstock\\.com'),\n",
    "    'sears': re.compile(r'sears\\.com'),\n",
    "    'kmart': re.compile(r'kmart\\.com'),\n",
    "    'macys': re.compile(r'macysassets\\.com'),\n",
    "    'staples': re.compile(r'staples\\.com'),\n",
    "    'Others': re.compile(r'')\n",
    "}\n",
    "\n",
    "def group_image_urls(data):\n",
    "    # Group URLs by merchant\n",
    "    grouped_urls = {merchant: [] for merchant in merchant_patterns.keys()}\n",
    "    for url in data[\"imageURLs\"]:\n",
    "        matched = False\n",
    "        for merchant, pattern in merchant_patterns.items():\n",
    "            if pattern.search(url):\n",
    "                grouped_urls[merchant].append(url)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            grouped_urls['Others'].append(url)\n",
    "\n",
    "    # Remove empty lists\n",
    "    grouped_urls = {k: v for k, v in grouped_urls.items() if v}\n",
    "    return grouped_urls\n",
    "\n",
    "def process_json_files(input_dir, output_dir):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Process each file in the input directory\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            input_file = os.path.join(input_dir, filename)\n",
    "            output_file = os.path.join(output_dir, filename)\n",
    "\n",
    "            # Read the input JSON file\n",
    "            with open(input_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Group the image URLs\n",
    "            grouped_urls = group_image_urls(data)\n",
    "\n",
    "            # Update the JSON data with grouped URLs\n",
    "            data[\"imageURLs\"] = grouped_urls\n",
    "\n",
    "            # Write the updated JSON data to the output file\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(data, f, indent=4)\n",
    "\n",
    "# Example usage\n",
    "input_dir = 'ImageURLs_JSONs'  # Replace with the path to your input directory\n",
    "output_dir = 'imageurls_jsons_v2'  # Replace with the path to your output directory\n",
    "process_json_files(input_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the folder paths\n",
    "input_folder_path = 'Output_JSONs'\n",
    "output_folder_path = 'Extracted_info_reviews'\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "# Initialize the counter for unknown EANs\n",
    "unknown_id_counter = 1\n",
    "\n",
    "# Iterate through all JSON files in the input folder\n",
    "for file_name in os.listdir(input_folder_path):\n",
    "    if file_name.endswith('.json'):  # Process only JSON files\n",
    "        file_path = os.path.join(input_folder_path, file_name)\n",
    "\n",
    "        # Load the JSON object from the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        # Extract the required fields\n",
    "        extracted_data = {\n",
    "            \"ean\": json_data.get(\"ean\"),\n",
    "            \"ean13\": json_data.get(\"ean13\"),\n",
    "            \"gtins\": json_data.get(\"gtins\"),\n",
    "            \"upc\": json_data.get(\"upc\"),\n",
    "            \"upca\": json_data.get(\"upca\"),\n",
    "            # Get asins from json_data.get(\"asins\") if it exists, otherwise use None\n",
    "            \"asins\": json_data.get(\"asins\", None),\n",
    "            \"id\": json_data.get(\"id\"),\n",
    "            \"reviews\": json_data.get(\"reviews\")\n",
    "        }\n",
    "\n",
    "        # Determine the file name using the EAN number or use a counter for unknown EANs\n",
    "        if extracted_data[\"id\"]:\n",
    "            id_number = extracted_data[\"id\"]\n",
    "        else:\n",
    "            id_number = f\"unknown_id_{unknown_id_counter}\"\n",
    "            unknown_id_counter += 1\n",
    "\n",
    "        # Save the extracted data to a new JSON file\n",
    "        output_file_path = os.path.join(output_folder_path, f'{id_number}.json')\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(extracted_data, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "        # Uncomment the line below if you want to print the output file path\n",
    "        # print(f'Extracted data saved to {output_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Run the python file\n",
    "subprocess.run([\"python\", \"convert_files_into_json.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DESCRIPTION PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Define the folder paths\n",
    "input_folder_path = 'Output_JSONs'\n",
    "output_folder_path = 'Extracted_info_descriptions'\n",
    "os.makedirs(output_folder_path, exist_ok=True)\n",
    "\n",
    "# Counter for unknown EAN files\n",
    "unknown_id_counter = 1\n",
    "\n",
    "# List to store information about files that created unknown_ean files\n",
    "unknown_id_files = []\n",
    "\n",
    "# Iterate through all JSON files in the input folder\n",
    "for file_name in os.listdir(input_folder_path):\n",
    "    if file_name.endswith('.json'):  # Process only JSON files\n",
    "        file_path = os.path.join(input_folder_path, file_name)\n",
    "\n",
    "        # Load the JSON object from the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        # Extract the required fields\n",
    "        extracted_data = {\n",
    "            \"brand\": json_data.get(\"brand\"),\n",
    "            \"descriptions\": [\n",
    "                {\n",
    "                    \"value\": desc.get(\"value\"),\n",
    "                    \"sourceURLs\": desc.get(\"sourceURLs\"),\n",
    "                    \"dateSeen\": desc.get(\"dateSeen\")\n",
    "                } for desc in json_data.get(\"descriptions\", [])\n",
    "            ],\n",
    "            \"dimension\": json_data.get(\"dimension\"),\n",
    "            \"domains\": json_data.get(\"domains\"),\n",
    "            \"ean\": json_data.get(\"ean\"),\n",
    "            \"ean13\": json_data.get(\"ean13\"),\n",
    "            \"gtins\": json_data.get(\"gtins\"),\n",
    "            \"upc\": json_data.get(\"upc\"),\n",
    "            \"upca\": json_data.get(\"upca\"),\n",
    "            \"id\": json_data.get(\"id\"),\n",
    "            \"warranty\": json_data.get(\"warranty\")\n",
    "        }\n",
    "\n",
    "        # Determine the output file name\n",
    "        if extracted_data[\"id\"]:\n",
    "            id_number = extracted_data[\"id\"]\n",
    "        else:\n",
    "            id_number = f\"unknown_id_{unknown_id_counter}\"\n",
    "            unknown_id_files.append({\"Input File\": file_name, \"Output File\": f'{id_number}.json'})\n",
    "            unknown_id_counter += 1\n",
    "\n",
    "            # Add the \"id\" field to extracted_data for unknown_ean files\n",
    "            #extracted_data[\"id\"] = json_data.get(\"id\")\n",
    "\n",
    "        output_file_path = os.path.join(output_folder_path, f'{id_number}.json')\n",
    "\n",
    "        # Save the extracted data to the output file\n",
    "        with open(output_file_path, 'w') as outfile:\n",
    "            json.dump(extracted_data, outfile, indent=4)\n",
    "\n",
    "        print(f'Extracted data saved to {output_file_path}')\n",
    "\n",
    "# Save the information about unknown EAN files to an Excel sheet\n",
    "if unknown_id_files:\n",
    "    df = pd.DataFrame(unknown_id_files)\n",
    "    df.to_excel('unknown_id_files.xlsx', index=False)\n",
    "    print('Excel sheet with unknown ID files created: unknown_id_files.xlsx')\n",
    "else:\n",
    "    print('No unknown EAN files were created.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
